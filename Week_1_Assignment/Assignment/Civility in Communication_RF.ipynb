{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Civility in Communication\n",
    "\n",
    "The focus of this assignment will be on a) training a classifier to perform hate speech detection; b) use LIME to explain the classifier's behaviour; c) establish whether the classifier might be biased wrt. different demographic dialects.\n",
    "\n",
    "This assignment is divided into three parts:\n",
    "1. **Before the laboratory** (individually): read [LIME's paper](https://arxiv.org/abs/1602.04938) and understand how its Python implementation works: https://github.com/marcotcr/lime (docs: https://lime-ml.readthedocs.io/en/latest/index.html). Check these tutorials in particular: [1](https://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html) and [2](https://marcotcr.github.io/lime/tutorials/Lime%20-%20multiclass.html). Furthermore, download the dataset, read its description below and make sure you understand it. Finally, implement a classifier to detect offensive language (use the \"label\" column in the train and dev datasets). You could for example use a TF-IDF model with any classifier you like from sklearn. Your focus, before the laboratory, is to clearly understand LIME and the proposed dataset, as well as to bring your own classifier to the laboratory.\n",
    "2. **During the laboratory** (in groups): compare your classifiers and chose one or two to work with (e.g., select the best-performing ones, or those using different methods). Split into two sub-groups: one will use LIME to come-up with explanations for classifications. In particular, they will focus on missclassifications and try to explain those. Another group will select a definition of bias (from literature - can be from week 2 or any other literature you find) and verify whether your classifier(s) are biased wrt. different demgraphic dialects. For this task, use your classifier(s) on the “mini_demographic_dev.tsv” dataset, and assess bias by demographic group (see below for details). At the end of the laboratory, try to combine your work by using LIME to explain biased classifications.\n",
    "3. **After the laboratory** (in groups): wrap-up your work and write up your results and thoughts into a brief project report. Make sure to discuss the question of whether you think LIME is effective at explaining your classifier(s), whether you found bias in the classifier, and whether LIME explains biased classifications well (or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "*This dataset and text is taken with permission from the [Computational Ethics for NLP course, HW2](http://demo.clab.cs.cmu.edu/ethical_nlp2020/homeworks/hw2/hw2.html).*\n",
    "\n",
    "The primary data for this assignment is available in the dataset folder. **Please note that the data contains offensive or sensitive content, including profanity and racial slurs.**\n",
    "\n",
    "We provide data drawn from two sources. The first (files \"train.tsv\" and \"dev.tsv\") consists of tweets annotated for offensiveness taken from the [2019 SemEval task](https://competitions.codalab.org/competitions/20011) on offensive language detection. In the files \"train.tsv\" and \"dev.tsv\", the first column (text) contains the text of a tweet, the second column (label) contains an offensiveness label:\n",
    "\n",
    "* (NOT) Not Offensive - This post does not contain offense or profanity.\n",
    "* (OFF) Offensive - This post contains offensive language or a targeted (veiled or direct) offense\n",
    "\n",
    "The file “offenseval-annotation.txt” provides additional details on the annotation scheme.\n",
    "\n",
    "We additionally provide a data set of tweets proxy-labelled for race in the file titled “mini_demographic_dev.tsv”. This data is taken from the [TwitterAAE](http://slanglab.cs.umass.edu/TwitterAAE/) data set and uses posterior proportions of demographic topics as a proxy for racial dialect ([details](https://www.aclweb.org/anthology/D16-1120.pdf)). The first column (“text”) contains the text of the tweet, and the second column (“demographic”) contains a label: “AA” (for “African American”), “White”, “Hispanic”, or “Other”. For this assignment, we assume that no tweet in the TwitterAAE data set contains toxic language. Thus, any tweet in this file that is classified as toxic is a false positive.\n",
    "\n",
    "Finally, both development sets (“dev.tsv” and “mini_demographic_dev.tsv”) contain a column “perspective_score”, which contains a toxicity score. These scores were obtain using the [PerspectiveAPI tool](https://www.perspectiveapi.com/) released by Alphabet. This tool is intended to help “developers and publishers…give realtime feedback to commenters or help moderators do their job”\n",
    "\n",
    "In all data sets, user mentions have been replaced with the token @USER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import nltk, sklearn\n",
    "\n",
    "import random\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"dataset/civility_data/train.tsv\", sep=\"\\t\")\n",
    "df_dev = pd.read_csv(\"dataset/civility_data/dev.tsv\", sep=\"\\t\")\n",
    "df_test = pd.read_csv(\"dataset/civility_data/test.tsv\", sep=\"\\t\")\n",
    "df_demo = pd.read_csv(\"dataset/civility_data/mini_demographic_dev.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10592, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@USER @USER You are an embarrassing citizen!!</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER Seems hard to believe that you stood nex...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@USER @USER @USER Wow !!! no wonder the Libera...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@USER @USER And not all idiots grandstands lik...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER Bring on the hypocrite gungrabber. MAGA</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label category\n",
       "0      @USER @USER You are an embarrassing citizen!!   OFF      TIN\n",
       "1  @USER Seems hard to believe that you stood nex...   OFF      TIN\n",
       "2  @USER @USER @USER Wow !!! no wonder the Libera...   OFF      TIN\n",
       "3  @USER @USER And not all idiots grandstands lik...   OFF      TIN\n",
       "4      @USER Bring on the hypocrite gungrabber. MAGA   OFF      TIN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>demographic</th>\n",
       "      <th>perspective_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>People make mistakes. It takes a good person t...</td>\n",
       "      <td>White</td>\n",
       "      <td>0.041031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Only one on our road with power, but no cable ...</td>\n",
       "      <td>White</td>\n",
       "      <td>0.061435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love when baby's yawn I think it's so cute.</td>\n",
       "      <td>White</td>\n",
       "      <td>0.056817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>theres so many hoes now that i actually think ...</td>\n",
       "      <td>White</td>\n",
       "      <td>0.503459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Today is the day Adalynn Alexis will be here! ...</td>\n",
       "      <td>White</td>\n",
       "      <td>0.092183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text demographic  \\\n",
       "0  People make mistakes. It takes a good person t...       White   \n",
       "1  Only one on our road with power, but no cable ...       White   \n",
       "2      I love when baby's yawn I think it's so cute.       White   \n",
       "3  theres so many hoes now that i actually think ...       White   \n",
       "4  Today is the day Adalynn Alexis will be here! ...       White   \n",
       "\n",
       "   perspective_score  \n",
       "0           0.041031  \n",
       "1           0.061435  \n",
       "2           0.056817  \n",
       "3           0.503459  \n",
       "4           0.092183  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier (Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a text classification model using character-level (`analyzer='char_wb'`) TF-IDF features (with 4–6 character n-grams; `ngram_range=(4,6)`), removing very rare n-grams that appear in fewer than 5 documents (`min_df=5`). A Random Forest classifier is then fitted on the training data, used to predict labels for the dev set, and evaluated using precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NOT       0.77      0.93      0.84       884\n",
      "         OFF       0.76      0.44      0.55       440\n",
      "\n",
      "    accuracy                           0.77      1324\n",
      "   macro avg       0.77      0.68      0.70      1324\n",
      "weighted avg       0.77      0.77      0.75      1324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train['text']\n",
    "y_train = df_train['label']\n",
    "\n",
    "X_dev = df_dev['text']\n",
    "y_dev = df_dev['label']\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='char_wb',\n",
    "    ngram_range=(4,6),\n",
    "    min_df=5,\n",
    "    max_features=300000\n",
    ")\n",
    "\n",
    "X_train_t = vectorizer.fit_transform(X_train)\n",
    "X_dev_t = vectorizer.transform(X_dev)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=1)\n",
    "model.fit(X_train_t, y_train)\n",
    "y_pred = model.predict(X_dev_t)\n",
    "\n",
    "print(classification_report(y_dev, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain with LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to explain:\n",
      " *a shooting happens in a country with gun control laws for the first time in literally decades*  Y'all: BUT THE GUN LAWS SHOULD'VE PREVENTED THIS UNGA BUNGA THIS IS WHY GUN CONTROL SUCKS BET YOU DUMMIES WISH YOU HAD GUNS  *meanwhile another school shooting happens in America*\n",
      "\n",
      "Random index:  994\n",
      "True label: NOT\n",
      "Predicted label: OFF\n",
      "\n",
      "LIME explanation:\n",
      "SUCKS: 0.292\n",
      "happens: -0.048\n",
      "shooting: -0.045\n",
      "PREVENTED: 0.043\n",
      "SHOULD: -0.017\n",
      "control: -0.015\n",
      "literally: -0.013\n",
      "country: 0.013\n",
      "decades: 0.011\n",
      "a: 0.008\n"
     ]
    }
   ],
   "source": [
    "# Function to predict probabilities for LIME\n",
    "def predict_proba_lime(texts):\n",
    "    X = vectorizer.transform(texts)\n",
    "    return model.predict_proba(X)\n",
    "\n",
    "class_names = model.classes_\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "# Select a random misclassified instance\n",
    "# random_index = random.choice(np.where(y_dev != y_pred)[0])\n",
    "# for the LIME explanation: \n",
    "random_index = 994\n",
    "\n",
    "text_instance = X_dev.iloc[random_index]\n",
    "print('Text to explain:\\n', text_instance)\n",
    "\n",
    "true_label = y_dev.iloc[random_index]\n",
    "pred_label = y_pred[random_index]\n",
    "\n",
    "print('\\nRandom index: ', random_index)\n",
    "print('True label:', true_label)\n",
    "print('Predicted label:', pred_label)\n",
    "\n",
    "# Generate LIME explanation\n",
    "exp = explainer.explain_instance(\n",
    "    text_instance,\n",
    "    predict_proba_lime,\n",
    "    num_features=10\n",
    ")\n",
    "\n",
    "# Display the explanation, which includes words and their weights\n",
    "print('\\nLIME explanation:')\n",
    "for feature, weight in exp.as_list():\n",
    "    print(f\"{feature}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After applying LIME to the misclassifications, we can see how the different words contribute to the misclassification. The example above shows the text which is misclassified as offensive where the true label is actually not offensive. The Lime explanation shows that word 'SUCKS' is more related to a offensive prediction, because of its positive weight. In particular, the word SUCKS is likely associated with negative contexts in the training data, which causes the model to predict offensive. At the same time, other words such as 'shooting' receive a negative weight and therefore slightly push the prediction toward the not offensive class, but this effect is probably much smaller. So in this case, the model chooses to predict offensive, based on the negative word 'SUCKS'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best word: sucks\n",
      "Removed feature: \"sucks\"\n",
      "Removed feature: \" sucks\"\n",
      "Removed feature: \"sucks \"\n",
      "\n",
      "Original prediction:\n",
      "  class = OFF, probability = 0.5800\n",
      "\n",
      "Prediction removing best word:\n",
      "  class = NOT, probability = 0.4800\n",
      "\n",
      "Difference in probability: -0.09999999999999998\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- Remove the most important feature ----\n",
    "\n",
    "# Get the feature weights from LIME explanation\n",
    "feature_weights = exp.as_list()\n",
    "feature_weights = sorted(feature_weights, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Identify the best word (most impactful feature)\n",
    "best_word = feature_weights[0][0].lower()\n",
    "print(f\"Best word: {best_word}\")\n",
    "\n",
    "tmp = vectorizer.transform([text_instance]).copy()\n",
    "\n",
    "# Consider different spacing variants of the best word\n",
    "variants = [\n",
    "    best_word,\n",
    "    f\" {best_word}\",\n",
    "    f\"{best_word} \"\n",
    "]\n",
    "\n",
    "# Remove the best word from the feature vector\n",
    "for v in variants:\n",
    "    if v in vectorizer.vocabulary_:\n",
    "        print(f'Removed feature: \"{v}\"')\n",
    "        tmp[0, vectorizer.vocabulary_[v]] = 0\n",
    "\n",
    "# ---- Predictions ----\n",
    "orig_vec = vectorizer.transform([text_instance])\n",
    "\n",
    "orig_proba = model.predict_proba(orig_vec)[0, 1]\n",
    "new_proba = model.predict_proba(tmp)[0, 1]\n",
    "\n",
    "orig_pred = model.predict(orig_vec)[0]\n",
    "new_pred = model.predict(tmp)[0]\n",
    "\n",
    "# Print the probabilities of the original and modified instances\n",
    "print('\\nOriginal prediction:')\n",
    "print(f'  class = {orig_pred}, probability = {orig_proba:.4f}\\n')\n",
    "print('Prediction removing best word:')\n",
    "print(f'  class = {new_pred}, probability = {new_proba:.4f}\\n')\n",
    "print('Difference in probability:', new_proba - orig_proba)\n",
    "\n",
    "# Visualize the LIME explanation\n",
    "fig = exp.as_pyplot_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After we got the LIME explanation, we tried to remove the word with the highest weight according to LIME. Then we predict the text again and we observe a clear drop in the predicted probability for the offensive class, and in this case even a change in the predicted label from offensive to not offensive. This also suggest that the word 'SUCKS' plays a strong local role in the model’s decision for this particular example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A biased classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier (Logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NOT       1.00      0.76      0.86      5072\n",
      "         OFF       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.76      5072\n",
      "   macro avg       0.50      0.38      0.43      5072\n",
      "weighted avg       1.00      0.76      0.86      5072\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "df_demo = df_demo.reset_index(drop=True)\n",
    "X_dev = df_demo['text']\n",
    "y_dev = ['NOT'] * len(df_demo)\n",
    "\n",
    "X_dev_t = vectorizer.transform(X_dev)\n",
    "\n",
    "model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=201\n",
    ")\n",
    "model.fit(X_train_t, y_train)\n",
    "y_pred = model.predict(X_dev_t)\n",
    "print(classification_report(y_dev, y_pred))\n",
    "\n",
    "df_demo[\"pred_label\"] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify if the classifier is biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution by counts\n",
      "demographic\n",
      "White       4235\n",
      "Hispanic     335\n",
      "AA           332\n",
      "Other        170\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribution by percentages\n",
      "demographic\n",
      "White       83.50\n",
      "Hispanic     6.60\n",
      "AA           6.55\n",
      "Other        3.35\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Distribution by counts\")\n",
    "print(df_demo['demographic'].value_counts())\n",
    "print()\n",
    "print(\"Distribution by percentages\")\n",
    "print((df_demo['demographic'].value_counts(normalize=True) * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positives (fp) by demographic:\n",
      "White   : count = 4235 | fp = 1019 | fp_rate 24.06%\n",
      "Hispanic: count =  335 | fp =  98 | fp_rate 29.25%\n",
      "AA      : count =  332 | fp = 115 | fp_rate 34.64%\n",
      "Other   : count =  170 | fp =   6 | fp_rate 3.53%\n"
     ]
    }
   ],
   "source": [
    "print(\"False positives (fp) by demographic:\")\n",
    "for group in df_demo['demographic'].unique():\n",
    "    n = len(df_demo[df_demo['demographic'] == group])\n",
    "    fp = (df_demo[df_demo['demographic'] == group]['pred_label'] == 'OFF').sum()\n",
    "\n",
    "    print(f\"{group:8}: count = {n:4} | fp = {fp:3} | fp_rate {fp/n:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain with LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to explain:\n",
      " Ain't do shit\n",
      "\n",
      "Random index:  5063\n",
      "True label: NOT\n",
      "Predicted label: OFF\n",
      "\n",
      "LIME explanation:\n",
      "shit: 0.629\n",
      "Ain: -0.035\n",
      "t: -0.019\n",
      "do: 0.004\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "aa_fp = df_demo[\n",
    "    (df_demo['demographic'] == 'AA') &\n",
    "    (df_demo['pred_label'] == 'OFF')\n",
    "]\n",
    "\n",
    "random_row = aa_fp.sample(n=1, random_state=None)\n",
    "random_index = random_row.index[0]\n",
    "\n",
    "text_instance = X_dev.iloc[random_index]\n",
    "print('Text to explain:\\n', text_instance)\n",
    "\n",
    "true_label = \"NOT\"\n",
    "pred_label = y_pred[random_index]\n",
    "\n",
    "print('\\nRandom index: ', random_index)\n",
    "print('True label:', true_label)\n",
    "print('Predicted label:', pred_label)\n",
    "\n",
    "exp = explainer.explain_instance(\n",
    "    text_instance,\n",
    "    predict_proba_lime,\n",
    "    num_features=10\n",
    ")\n",
    "\n",
    "print('\\nLIME explanation:')\n",
    "for feature, weight in exp.as_list():\n",
    "    print(f\"{feature}: {weight:.3f}\")\n",
    "    \n",
    "fig = exp.as_pyplot_figure()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
